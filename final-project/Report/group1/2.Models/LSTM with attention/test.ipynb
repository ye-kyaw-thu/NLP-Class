{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from attention import AttentionLayer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import LSTM, Input, Dense,Embedding, Concatenate, TimeDistributed\n",
    "from tensorflow.keras.models import Model,load_model, model_from_json\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.preprocessing.text import one_hot, Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import pickle as pkl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pos-train\",'r') as f:\n",
    "  data_train = f.read()\n",
    "uncleaned_data_list_train = data_train.split('\\n')\n",
    "# Training Data\n",
    "burmese_train= []\n",
    "pos_train = []\n",
    "for word in uncleaned_data_list_train:\n",
    "  word = word.strip().split(\"<|||>\")\n",
    "  if(len(word) == 2):\n",
    "      burmese_train.append(word[0])\n",
    "      pos_train.append(word[1])\n",
    "# Putting the start and end words in the marathi sentances\n",
    "pos_train = [\"start \" + x.strip() + \" end\" for x in pos_train]\n",
    "burmeseTokenizer = Tokenizer(oov_token=\"OOV\")\n",
    "burmeseTokenizer.fit_on_texts(burmese_train)\n",
    "\n",
    "Bword2index = burmeseTokenizer.word_index\n",
    "vocab_size_source = len(Bword2index) + 1\n",
    "posTokenizer = Tokenizer(oov_token=\"OOV\")\n",
    "\n",
    "posTokenizer.fit_on_texts(pos_train)\n",
    "Pword2index = posTokenizer.word_index\n",
    "vocab_size_target = len(Pword2index) + 1\n",
    "\n",
    "# loading the model architecture and asigning the weights\n",
    "json_file = open('NMT_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model_loaded = model_from_json(loaded_model_json, custom_objects={'AttentionLayer': AttentionLayer})\n",
    "# load weights into new model\n",
    "model_loaded.load_weights(\"NMT_model_weight.h5\")\n",
    "\n",
    "with open('NMT_Btokenizer.pkl','rb') as f:\n",
    "    vocab_size_source, Bword2index, Bindex2word,burmeseTokenizer = pkl.load(f)\n",
    "\n",
    "with open('NMT_Ptokenizer.pkl', 'rb') as f:\n",
    "    vocab_size_target, Pword2index, Pindex2word,posTokenizer = pkl.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 150, 500)\n",
      "(None, None)\n"
     ]
    }
   ],
   "source": [
    "latent_dim=500\n",
    "# encoder inference\n",
    "encoder_inputs = model_loaded.input[0]  #loading encoder_inputs\n",
    "encoder_outputs, state_h, state_c = model_loaded.layers[6].output #loading encoder_outputs\n",
    "\n",
    "print(encoder_outputs.shape)\n",
    "\n",
    "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# decoder inference\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_hidden_state_input = Input(shape=(150,latent_dim))\n",
    "\n",
    "# Get the embeddings of the decoder sequence\n",
    "decoder_inputs = model_loaded.layers[3].output\n",
    "\n",
    "print(decoder_inputs.shape)\n",
    "dec_emb_layer = model_loaded.layers[5]\n",
    "\n",
    "dec_emb2= dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_lstm = model_loaded.layers[7]\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "#attention inference\n",
    "attn_layer = model_loaded.layers[8]\n",
    "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
    "\n",
    "concate = model_loaded.layers[9]\n",
    "decoder_inf_concat = concate([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "decoder_dense = model_loaded.layers[10]\n",
    "decoder_outputs2 = decoder_dense(decoder_inf_concat)\n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "[decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "[decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence_attention(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "\n",
    "    # Chose the 'start' word as the first word of the target sequence\n",
    "    target_seq[0, 0] = Pword2index['start']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        if sampled_token_index == 0:\n",
    "          break\n",
    "        else:\n",
    "          sampled_token = Pindex2word[sampled_token_index]\n",
    "\n",
    "          if(sampled_token!='end'):\n",
    "              decoded_sentence += ' '+sampled_token\n",
    "\n",
    "              # Exit condition: either hit max length or find stop word.\n",
    "              if (sampled_token == 'end' or len(decoded_sentence.split()) >= (26-1)):\n",
    "                  stop_condition = True\n",
    "\n",
    "          # Update the target sequence (of length 1).\n",
    "          target_seq = np.zeros((1,1))\n",
    "          target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "          # Update internal states\n",
    "          e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = [\"ကျွန်တော် ဖြိုးသူထက် ပါ ။\"]\n",
    "input_data = input_data\n",
    "test =burmeseTokenizer.texts_to_sequences(input_data)\n",
    "test = pad_sequences(test, maxlen=150, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Input:\",input_data)\n",
    "print(\"LSTM with attention:\", decode_sequence_attention(test.reshape(1,150)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
